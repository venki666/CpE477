{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Exercise: Minimizing Loss\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/edgeimpulse/courseware-embedded-machine-learning/blob/main/Module%202%20-%20Getting%20Started%20with%20Deep%20Learning/2.1.8.exploring-loss.3.ipynb)\n",
        "\n",
        "In this next Colab you will get to minimize the loss function for our linear regression model using TensorFlowâ€™s built-in GradientTape class. You will also get to visualize the training process, seeing how both the bias and weight converge on the optimal solution across training epochs."
      ],
      "metadata": {
        "id": "pilN_2fQNHze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtmQDB6EjReV"
      },
      "outputs": [],
      "source": [
        "# First import the functions we will need\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5-58WYokl3"
      },
      "source": [
        "## GradientTape\n",
        "\n",
        "The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdhgbGE2j02L"
      },
      "outputs": [],
      "source": [
        "# Define our initial guess\n",
        "INITIAL_W = 10.0\n",
        "INITIAL_B = 10.0\n",
        "\n",
        "# Define our loss function\n",
        "def loss(predicted_y, target_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))\n",
        "\n",
        "# Define our training procedure\n",
        "def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "    # Here is where you differentiate the model values with respect to the loss function\n",
        "    dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "    # And here is where you update the model values based on the learning rate chosen\n",
        "    model.w.assign_sub(learning_rate * dw)\n",
        "    model.b.assign_sub(learning_rate * db)\n",
        "    return current_loss\n",
        "\n",
        "# Define our simple linear regression model\n",
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    # Initialize the weights\n",
        "    self.w = tf.Variable(INITIAL_W)\n",
        "    self.b = tf.Variable(INITIAL_B)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl5YGfySNCDX"
      },
      "source": [
        "### Train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8akmDCoAjgak"
      },
      "outputs": [],
      "source": [
        "# Define our input data and learning rate\n",
        "xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]\n",
        "LEARNING_RATE=0.09\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "# Collect the history of w-values and b-values to plot later\n",
        "list_w, list_b = [], []\n",
        "epochs = range(50)\n",
        "losses = []\n",
        "for epoch in epochs:\n",
        "  list_w.append(model.w.numpy())\n",
        "  list_b.append(model.b.numpy())\n",
        "  current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)\n",
        "  losses.append(current_loss)\n",
        "  print('Epoch %2d: w=%1.2f b=%1.2f, loss=%2.5f' %\n",
        "        (epoch, list_w[-1], list_b[-1], current_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUruUzwNCDY"
      },
      "source": [
        "### Plot our trained values over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGgb5grSk8Ax"
      },
      "outputs": [],
      "source": [
        "# Plot the w-values and b-values for each training Epoch against the true values\n",
        "TRUE_w = 2.0\n",
        "TRUE_b = -1.0\n",
        "plt.plot(epochs, list_w, 'r', epochs, list_b, 'b')\n",
        "plt.plot([TRUE_w] * len(epochs), 'r--', [TRUE_b] * len(epochs), 'b--')\n",
        "plt.legend(['w', 'b', 'True w', 'True b'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Mimimizing-Loss.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
