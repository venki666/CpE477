{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanActivityClassification",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ZvUOo_TjxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXsT7lB_Wowl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUq0Y3E8WTdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix = '/content/drive/My Drive/HumanActivityDatasetSplit(3000)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2me_FW_Wtay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_longest_df(directory):\n",
        "    fullfilepaths = list() #list of each files(the full path of file)\n",
        "    \n",
        "    filegroups = ['/dws_1','/dws_2','/dws_11','/jog_9','/jog_16','/sit_5','/sit_13',\n",
        "                  '/std_6','/std_14','/ups_3','/ups_4','/ups_12','/wlk_7','/wlk_8','/wlk_15']\n",
        "    \n",
        "    if (directory == prefix + '/train' ):\n",
        "        namesOfFiles = ['/sub_1.csv','/sub_2.csv','/sub_3.csv','/sub_4.csv','/sub_5.csv',\n",
        "                        '/sub_6.csv','/sub_7.csv','/sub_8.csv','/sub_9.csv','/sub_10.csv',\n",
        "                        '/sub_11.csv','/sub_12.csv','/sub_13.csv','/sub_14.csv','/sub_15.csv',\n",
        "                        '/sub_16.csv','/sub_17.csv']\n",
        "    else:\n",
        "         namesOfFiles = ['/sub_18.csv','/sub_19.csv','/sub_20.csv' ,\n",
        "                         '/sub_21.csv','/sub_22.csv',\n",
        "                         '/sub_23.csv','/sub_24.csv']\n",
        "    #loop will put each files full path in a list\n",
        "    for group in filegroups:\n",
        "        filepath = directory + group \n",
        "        for file in namesOfFiles:\n",
        "            fullfilepaths.append(filepath + file)\n",
        "    #return fullfilepaths\n",
        "    length_of_files = list()\n",
        "    for file in fullfilepaths:\n",
        "        df = pd.read_csv(file)\n",
        "        length_of_files.append(len(df))\n",
        "    return sorted(length_of_files)[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5FJnJ_WvqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#variables to store length of longest files\n",
        "#longest_file_train = find_longest_df(prefix+'/train')\n",
        "#longest_file_test = find_longest_df(prefix+'/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1WifWS7YyPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#longest_file_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB3rlAjEY2dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#longest_file_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDs-dD97Tles",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#returns file into a numpy array\n",
        "def load_file(filepath,train=False):\n",
        "    #if(train==True):\n",
        "    #    longest_file = longest_file_train\n",
        "     # else:\n",
        "    #    longest_file = longest_file_test\n",
        "    #print('Train is ' + str(train))\n",
        "    #print('longest file is ' + str(longest_file))\n",
        "    #dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    dataframe = pd.read_csv(filepath)\n",
        "    #dataframe = dataframe.drop(labels='Unnamed: 0',axis=1)\n",
        "    dataframe = dataframe.drop(dataframe.columns[[0]],axis=1)\n",
        "    ##\n",
        "    #pad the length of dataframe to be length of file with most timesteps\n",
        "    #empty_row = pd.DataFrame([[0]*dataframe.shape[1]],columns=dataframe.columns)\n",
        "    #while(len(dataframe) <longest_file):\n",
        "    #    dataframe = dataframe.append(empty_row,ignore_index=True)\n",
        "    ##\n",
        "    #dataframe = dataframe[:300] #we are only getting first 300 time steps for consistency\n",
        "    return dataframe.values.reshape(1,len(dataframe),6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969PIJZGV3KR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_file_y(filepath):\n",
        "    dataframe = pd.read_csv(filepath)\n",
        "    return dataframe.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w292MA-9WM53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_each_file(filepaths, prefix='',train=False):\n",
        "    #count = 0\n",
        "    loaded = list()\n",
        "    for file in filepaths:\n",
        "        data = load_file(file,train=train)\n",
        "        #count = count+1\n",
        "        #print(str(count))\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = np.vstack(loaded)\n",
        "    return loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIlHBMSnW4QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset_group(directory):\n",
        "    fullfilepaths = list() #list of each files(the full path of file)\n",
        "    \n",
        "    filegroups = ['/dws_1','/dws_2','/dws_11','/jog_9','/jog_16','/sit_5','/sit_13',\n",
        "                  '/std_6','/std_14','/ups_3','/ups_4','/ups_12','/wlk_7','/wlk_8','/wlk_15']\n",
        "    \n",
        "    if (directory == prefix + '/train' ):\n",
        "        namesOfFiles = ['/sub_1.csv','/sub_2.csv','/sub_3.csv','/sub_4.csv','/sub_5.csv',\n",
        "                        '/sub_6.csv','/sub_7.csv','/sub_8.csv','/sub_9.csv','/sub_10.csv',\n",
        "                        '/sub_11.csv','/sub_12.csv','/sub_13.csv','/sub_14.csv','/sub_15.csv',\n",
        "                        '/sub_16.csv','/sub_17.csv']\n",
        "        train = True\n",
        "    else:\n",
        "        train = False\n",
        "        namesOfFiles = ['/sub_18.csv','/sub_19.csv','/sub_20.csv' ,\n",
        "                         '/sub_21.csv','/sub_22.csv',\n",
        "                         '/sub_23.csv','/sub_24.csv']\n",
        "    #loop will put each files full path in a list\n",
        "    for group in filegroups:\n",
        "        filepath = directory + group \n",
        "        for file in namesOfFiles:\n",
        "            fullfilepaths.append(filepath + file)\n",
        "            \n",
        "    X = load_each_file(fullfilepaths,train=train)\n",
        "    if (directory == prefix + '/train'):\n",
        "        y = load_file_y(directory + '/y_train.txt')\n",
        "    else:\n",
        "        y = load_file_y(directory + '/y_test.txt')\n",
        "    return X, y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJmbmHN8W4sL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#goes into train/test directory\n",
        "def load_dataset(directory=prefix):\n",
        "    trainX,trainy = load_dataset_group(directory + '/train')\n",
        "    testX,testy = load_dataset_group(directory + '/test')\n",
        "    print(trainX.shape, trainy.shape)\n",
        "    print(testX.shape, testy.shape)\n",
        "    \n",
        "    trainy = to_categorical(trainy)\n",
        "    testy = to_categorical(testy)\n",
        "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "    return trainX,trainy,testX,testy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTijUOzzW9xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "    verbose, epochs, batch_size = 0, 30, 4\n",
        "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "    # evaluate model\n",
        "    #losses = pd.DataFrame(model.history.history)\n",
        "    #losses.plot()\n",
        "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "    return accuracy'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxOOGhLGd1xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tverbose, epochs, batch_size = 0, 50, 64\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# reshape data into time steps of sub-sequences\n",
        "\tn_steps, n_length = 1, 3000\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
        "\ttestX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "\tmodel.add(TimeDistributed(Dropout(0.5)))\n",
        "\tmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "\tmodel.add(TimeDistributed(Flatten()))\n",
        "\tmodel.add(LSTM(100))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\tpredictions = model.predict_classes(testX)\n",
        "\tprint(predictions)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcw-ZZESO9fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit and evaluate a model\n",
        "'''\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tverbose, epochs, batch_size = 0, 50, 64\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "\tn_steps, n_length = 1, 3000\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\ttestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ3-K00AW-JK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2CnRSwlW-kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_experiment(repeats=10):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        " # summarize results\n",
        "\tsummarize_results(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juGdglwdZWfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def run_experiment(repeats=10):\n",
        "  #load data\n",
        "  trainX,trainy,testX,testy = load_dataset()\n",
        "  scaler = MinMaxScaler()\n",
        "  for i in range(trainX.shape[0]):\n",
        "    scaler.fit(trainX[i])\n",
        "    trainX[i] = scaler.transform(trainX[i])\n",
        "  for i in range(testX.shape[0]):\n",
        "    testX[i] = scaler.transform(testX[i])\n",
        "  #repeat experiment\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX,trainy,testX,testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1,score))\n",
        "    scores.append(score)\n",
        "  summarize_results(scores)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTiBfd28avSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_experiment() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfgg77n3rO_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}